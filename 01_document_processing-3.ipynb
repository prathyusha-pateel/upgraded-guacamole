{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Processing Pipeline\n",
    "## Extract and Chunk PDFs using Amazon Textract Textractor and LangChain\n",
    "\n",
    "**Purpose**: Process 6 sample PDF documents through Textractor extraction and LangChain recursive splitting to prepare for BM25 indexing.\n",
    "\n",
    "**Library**: Uses `amazon-textract-textractor` for layout-aware text extraction\n",
    "\n",
    "**Outputs**: \n",
    "- Raw extracted text (saved as checkpoint)\n",
    "- Post-processed text (saved as checkpoint)\n",
    "- Processed chunks with metadata (ready for indexing)\n",
    "\n",
    "**Next Step**: `02_indexing.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Amazon Textract Textractor library\n",
    "from textractor import Textractor\n",
    "from textractor.data.constants import TextractFeatures\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Logging setup\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úì Imports completed\")\n",
    "print(\"  - Using amazon-textract-textractor library for extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AWS clients\n",
    "session = boto3.Session()\n",
    "s3_client = session.client('s3')\n",
    "\n",
    "# Verify credentials\n",
    "sts = session.client('sts')\n",
    "identity = sts.get_caller_identity()\n",
    "print(f\"‚úì AWS Identity: {identity['Arn']}\")\n",
    "print(f\"‚úì Region: {session.region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration\n",
    "\n",
    "Centralize all parameters for reproducibility and easy experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessingConfig:\n",
    "    \"\"\"Configuration for document processing pipeline.\"\"\"\n",
    "    \n",
    "    # Document sources\n",
    "    PDF_SOURCE_TYPE = \"s3\"  # or \"local\"\n",
    "    S3_BUCKET = \"your-bucket-name\"\n",
    "    S3_PREFIX = \"raw-pdfs/\"\n",
    "    LOCAL_PDF_DIR = \"./data/pdfs/\"\n",
    "    \n",
    "    # Output paths\n",
    "    CHECKPOINT_DIR = \"./checkpoints/\"\n",
    "    RAW_EXTRACTION_FILE = \"raw_extractions.json\"\n",
    "    PROCESSED_EXTRACTIONS_FILE = \"processed_extractions.json\"\n",
    "    PROCESSED_CHUNKS_FILE = \"processed_chunks.json\"\n",
    "    \n",
    "    # Textract settings (using amazon-textract-textractor)\n",
    "    # Features: LAYOUT, TABLES, FORMS, SIGNATURES, QUERIES\n",
    "    TEXTRACT_FEATURES = [\"LAYOUT\", \"TABLES\"]  # List of feature names as strings\n",
    "    \n",
    "    # Chunking parameters\n",
    "    CHUNK_SIZE = 1000\n",
    "    CHUNK_OVERLAP = 200\n",
    "    SEPARATORS = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    LENGTH_FUNCTION = len\n",
    "    \n",
    "    # Processing\n",
    "    BATCH_SIZE = 6  # Number of documents\n",
    "    PROCESSING_TIMESTAMP = datetime.utcnow().isoformat()\n",
    "    \n",
    "    @classmethod\n",
    "    def to_dict(cls) -> Dict[str, Any]:\n",
    "        \"\"\"Export config as dictionary for logging.\"\"\"\n",
    "        return {\n",
    "            k: v for k, v in cls.__dict__.items() \n",
    "            if not k.startswith('_') and not callable(v)\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def save(cls, filepath: str):\n",
    "        \"\"\"Save configuration to JSON.\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(cls.to_dict(), f, indent=2)\n",
    "\n",
    "# Display current configuration\n",
    "config_df = pd.DataFrame([\n",
    "    {\"Parameter\": k, \"Value\": v} \n",
    "    for k, v in ProcessingConfig.to_dict().items()\n",
    "])\n",
    "print(\"\\nüìã Current Configuration:\")\n",
    "display(config_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directory\n",
    "checkpoint_dir = Path(ProcessingConfig.CHECKPOINT_DIR)\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save config for reproducibility\n",
    "ProcessingConfig.save(checkpoint_dir / \"config.json\")\n",
    "print(f\"‚úì Configuration saved to {checkpoint_dir / 'config.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Processing Classes\n",
    "\n",
    "Three separate classes for extraction, processing, and chunking.\n",
    "\n",
    "**Design Pattern:**\n",
    "- `TextExtractor`: PDF ‚Üí Raw text (via Textractor)\n",
    "- `TextProcessor`: Raw text ‚Üí Cleaned/processed text\n",
    "- `TextChunker`: Processed text ‚Üí Chunks\n",
    "\n",
    "This separation allows independent testing, customization, and checkpointing at each stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 TextExtractor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextExtractor:\n",
    "    \"\"\"\n",
    "    Handles PDF extraction using amazon-textract-textractor library.\n",
    "    \n",
    "    The Textractor library provides:\n",
    "    - High-level interface to Textract async API (start_document_analysis)\n",
    "    - Automatic polling and result retrieval\n",
    "    - Built-in parsing of Textract responses\n",
    "    - Access to layout elements (titles, headers, paragraphs, tables, etc.)\n",
    "    - Linearized text in reading order\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        \n",
    "        # Initialize Textractor\n",
    "        # Will use the default boto3 session/credentials\n",
    "        self.extractor = Textractor(profile_name=None)\n",
    "        \n",
    "    def extract_from_pdf(self, pdf_path: str, doc_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract text from PDF using Textractor library.\n",
    "        \n",
    "        Textractor handles:\n",
    "        - Calling start_document_analysis (async) or analyze_document (sync)\n",
    "        - Polling for job completion\n",
    "        - Retrieving and parsing all result pages\n",
    "        - Organizing text in reading order\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: S3 URI (s3://bucket/key) or local file path\n",
    "            doc_id: Unique identifier for this document\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with extracted text and metadata\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Extracting text from: {pdf_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Determine which features to use\n",
    "            features = [\n",
    "                getattr(TextractFeatures, f) \n",
    "                for f in self.config.TEXTRACT_FEATURES\n",
    "            ]\n",
    "            \n",
    "            self.logger.info(f\"Using Textract features: {self.config.TEXTRACT_FEATURES}\")\n",
    "            \n",
    "            # Call Textractor - it handles async API automatically\n",
    "            if pdf_path.startswith('s3://'):\n",
    "                # S3 path - use start_document_analysis (async)\n",
    "                self.logger.info(\"Starting async document analysis...\")\n",
    "                document = self.extractor.start_document_analysis(\n",
    "                    file_source=pdf_path,\n",
    "                    features=features,\n",
    "                    save_image=False  # Don't save image overlays in POC\n",
    "                )\n",
    "            else:\n",
    "                # Local file - can use analyze_document (sync) for small files\n",
    "                self.logger.info(\"Processing local file with analyze_document (sync)...\")\n",
    "                document = self.extractor.analyze_document(\n",
    "                    file_source=pdf_path,\n",
    "                    features=features,\n",
    "                    save_image=False\n",
    "                )\n",
    "            \n",
    "            self.logger.info(\"‚úì Document analysis complete, parsing results...\")\n",
    "            \n",
    "            # Extract page-level information\n",
    "            pages_data = []\n",
    "            for page_num, page in enumerate(document.pages, start=1):\n",
    "                # Get text for this page using Textractor's linearization\n",
    "                # This respects reading order based on layout\n",
    "                page_text = page.get_text()\n",
    "                \n",
    "                # Get layout information if available\n",
    "                layout_elements = {}\n",
    "                if hasattr(page, 'page_layout') and page.page_layout:\n",
    "                    layout = page.page_layout\n",
    "                    # Count different layout element types\n",
    "                    layout_elements = {\n",
    "                        'titles': len(layout.titles) if hasattr(layout, 'titles') else 0,\n",
    "                        'headers': len(layout.headers) if hasattr(layout, 'headers') else 0,\n",
    "                        'sections': len(layout.section_headers) if hasattr(layout, 'section_headers') else 0,\n",
    "                        'paragraphs': len(layout.texts) if hasattr(layout, 'texts') else 0,\n",
    "                        'tables': len(layout.tables) if hasattr(layout, 'tables') else 0,\n",
    "                        'lists': len(layout.lists) if hasattr(layout, 'lists') else 0,\n",
    "                        'figures': len(layout.figures) if hasattr(layout, 'figures') else 0,\n",
    "                    }\n",
    "                \n",
    "                pages_data.append({\n",
    "                    'page_number': page_num,\n",
    "                    'text': page_text,\n",
    "                    'char_count': len(page_text),\n",
    "                    'layout_elements': layout_elements\n",
    "                })\n",
    "            \n",
    "            # Compile full document text\n",
    "            # Using Textractor's get_text() which provides linearized text\n",
    "            full_text = document.get_text()\n",
    "            \n",
    "            # Get table information\n",
    "            tables_info = []\n",
    "            for table_idx, table in enumerate(document.tables):\n",
    "                tables_info.append({\n",
    "                    'table_index': table_idx,\n",
    "                    'page': table.page if hasattr(table, 'page') else None,\n",
    "                    'rows': table.n_rows if hasattr(table, 'n_rows') else 0,\n",
    "                    'cols': table.n_cols if hasattr(table, 'n_cols') else 0,\n",
    "                })\n",
    "            \n",
    "            result = {\n",
    "                'doc_id': doc_id,\n",
    "                'doc_name': Path(pdf_path).name,\n",
    "                'source_path': pdf_path,\n",
    "                'full_text': full_text.strip(),\n",
    "                'pages': pages_data,\n",
    "                'page_count': len(pages_data),\n",
    "                'total_char_count': len(full_text),\n",
    "                'tables': tables_info,\n",
    "                'table_count': len(tables_info),\n",
    "                'extraction_timestamp': datetime.utcnow().isoformat(),\n",
    "                'processed': False,  # Not yet post-processed\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "            self.logger.info(\n",
    "                f\"‚úì Extracted {len(pages_data)} pages, \"\n",
    "                f\"{len(full_text):,} characters, \"\n",
    "                f\"{len(tables_info)} tables\"\n",
    "            )\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚úó Extraction failed: {str(e)}\")\n",
    "            import traceback\n",
    "            self.logger.error(traceback.format_exc())\n",
    "            \n",
    "            return {\n",
    "                'doc_id': doc_id,\n",
    "                'doc_name': Path(pdf_path).name,\n",
    "                'source_path': pdf_path,\n",
    "                'status': 'failed',\n",
    "                'error': str(e),\n",
    "                'extraction_timestamp': datetime.utcnow().isoformat()\n",
    "            }\n",
    "    \n",
    "    def extract_batch(self, pdf_sources: List[tuple]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract text from multiple PDFs.\n",
    "        \n",
    "        Note: This processes sequentially. For production:\n",
    "        - Submit all jobs first (store job IDs)\n",
    "        - Poll all jobs together\n",
    "        - Process results as they complete\n",
    "        \n",
    "        Args:\n",
    "            pdf_sources: List of (pdf_path, doc_id) tuples\n",
    "            \n",
    "        Returns:\n",
    "            List of extraction result dictionaries\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Extracting batch of {len(pdf_sources)} documents\")\n",
    "        \n",
    "        results = []\n",
    "        for pdf_path, doc_id in pdf_sources:\n",
    "            result = self.extract_from_pdf(pdf_path, doc_id)\n",
    "            results.append(result)\n",
    "        \n",
    "        success_count = sum(1 for r in results if r['status'] == 'success')\n",
    "        self.logger.info(\n",
    "            f\"‚úì Extraction complete: {success_count}/{len(pdf_sources)} successful\"\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"‚úì TextExtractor class defined (using amazon-textract-textractor library)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 TextProcessor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TextProcessor:\n",
    "    \"\"\"\n",
    "    Handles text cleaning and post-processing after extraction.\n",
    "    \n",
    "    This is where you add custom logic for:\n",
    "    - Cleaning OCR artifacts\n",
    "    - Removing headers/footers\n",
    "    - Fixing hyphenation\n",
    "    - Normalizing whitespace\n",
    "    - Any domain-specific processing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    def process(self, extraction_result: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Apply all post-processing steps to extracted text.\n",
    "        \n",
    "        Args:\n",
    "            extraction_result: Output from TextExtractor.extract_from_pdf()\n",
    "            \n",
    "        Returns:\n",
    "            Modified extraction_result with processed text\n",
    "        \"\"\"\n",
    "        if extraction_result['status'] != 'success':\n",
    "            self.logger.warning(f\"Skipping processing for failed extraction: {extraction_result['doc_id']}\")\n",
    "            return extraction_result\n",
    "        \n",
    "        self.logger.info(f\"Processing text for: {extraction_result['doc_id']}\")\n",
    "        \n",
    "        try:\n",
    "            text = extraction_result['full_text']\n",
    "            original_length = len(text)\n",
    "            \n",
    "            # Apply processing pipeline\n",
    "            processing_steps = []\n",
    "            \n",
    "            # Step 1: Clean whitespace\n",
    "            text = self._clean_whitespace(text)\n",
    "            processing_steps.append('clean_whitespace')\n",
    "            \n",
    "            # Step 2: Fix hyphenation (words split across lines)\n",
    "            text = self._fix_hyphenation(text)\n",
    "            processing_steps.append('fix_hyphenation')\n",
    "            \n",
    "            # Step 3: Remove common headers/footers patterns\n",
    "            text = self._remove_headers_footers(text)\n",
    "            processing_steps.append('remove_headers_footers')\n",
    "            \n",
    "            # Step 4: Normalize unicode characters\n",
    "            text = self._normalize_unicode(text)\n",
    "            processing_steps.append('normalize_unicode')\n",
    "            \n",
    "            # Step 5: Remove page numbers\n",
    "            text = self._remove_page_numbers(text)\n",
    "            processing_steps.append('remove_page_numbers')\n",
    "            \n",
    "            # ADD YOUR CUSTOM PROCESSING STEPS HERE\n",
    "            # text = self._your_custom_method(text)\n",
    "            # processing_steps.append('your_custom_step')\n",
    "            \n",
    "            # Update extraction result\n",
    "            extraction_result['full_text'] = text\n",
    "            extraction_result['processed'] = True\n",
    "            extraction_result['processing_steps'] = processing_steps\n",
    "            extraction_result['processing_timestamp'] = datetime.utcnow().isoformat()\n",
    "            extraction_result['char_count_after_processing'] = len(text)\n",
    "            extraction_result['chars_removed'] = original_length - len(text)\n",
    "            \n",
    "            self.logger.info(\n",
    "                f\"‚úì Processed: {original_length:,} ‚Üí {len(text):,} chars \"\n",
    "                f\"({extraction_result['chars_removed']:,} removed)\"\n",
    "            )\n",
    "            \n",
    "            return extraction_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚úó Processing failed: {str(e)}\")\n",
    "            extraction_result['status'] = 'processing_failed'\n",
    "            extraction_result['error'] = str(e)\n",
    "            return extraction_result\n",
    "    \n",
    "    def _clean_whitespace(self, text: str) -> str:\n",
    "        \"\"\"Remove excessive whitespace while preserving paragraph structure.\"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        # Replace more than 2 newlines with exactly 2 (paragraph break)\n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "        # Remove trailing/leading whitespace per line\n",
    "        text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
    "        return text.strip()\n",
    "    \n",
    "    def _fix_hyphenation(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Fix words split across lines with hyphens.\n",
    "        Example: 'exam-\\nple' ‚Üí 'example'\n",
    "        \"\"\"\n",
    "        # Pattern: word character, hyphen, newline, word character\n",
    "        text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "        return text\n",
    "    \n",
    "    def _remove_headers_footers(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove common header/footer patterns.\n",
    "        \n",
    "        NOTE: This is a simple heuristic. For production, you may want:\n",
    "        - Page-aware processing (using extraction_result['pages'])\n",
    "        - Machine learning based detection\n",
    "        - Custom patterns based on your document types\n",
    "        \"\"\"\n",
    "        # Remove standalone page numbers (number on its own line)\n",
    "        text = re.sub(r'^\\d+$', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove common footer patterns like \"Page X of Y\"\n",
    "        text = re.sub(r'Page \\d+ of \\d+', '', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _normalize_unicode(self, text: str) -> str:\n",
    "        \"\"\"Normalize unicode characters to standard forms.\"\"\"\n",
    "        # Replace smart quotes with standard quotes\n",
    "        replacements = {\n",
    "            '\\u2018': \"'\",  # Left single quote\n",
    "            '\\u2019': \"'\",  # Right single quote\n",
    "            '\\u201c': '\"',  # Left double quote\n",
    "            '\\u201d': '\"',  # Right double quote\n",
    "            '\\u2013': '-',  # En dash\n",
    "            '\\u2014': '-',  # Em dash\n",
    "            '\\u2026': '...', # Ellipsis\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _remove_page_numbers(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove standalone page numbers that appear in text.\n",
    "        Be careful not to remove legitimate numbers!\n",
    "        \"\"\"\n",
    "        # Remove patterns like: \"\\n5\\n\" or \"\\nPage 5\\n\"\n",
    "        text = re.sub(r'\\n\\s*Page\\s+\\d+\\s*\\n', '\\n', text, flags=re.IGNORECASE)\n",
    "        return text\n",
    "    \n",
    "    def process_batch(self, extraction_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process multiple extraction results.\n",
    "        \n",
    "        Args:\n",
    "            extraction_results: List of extraction result dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            List of processed extraction results\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Processing batch of {len(extraction_results)} documents\")\n",
    "        \n",
    "        processed_results = []\n",
    "        for extraction in extraction_results:\n",
    "            processed = self.process(extraction)\n",
    "            processed_results.append(processed)\n",
    "        \n",
    "        success_count = sum(1 for r in processed_results if r.get('processed', False))\n",
    "        self.logger.info(f\"‚úì Processing complete: {success_count}/{len(extraction_results)} successful\")\n",
    "        \n",
    "        return processed_results\n",
    "\n",
    "print(\"‚úì TextProcessor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 TextChunker Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextChunker:\n",
    "    \"\"\"Handles text chunking using LangChain RecursiveCharacterTextSplitter.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        \n",
    "        # Initialize text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=config.CHUNK_SIZE,\n",
    "            chunk_overlap=config.CHUNK_OVERLAP,\n",
    "            separators=config.SEPARATORS,\n",
    "            length_function=config.LENGTH_FUNCTION,\n",
    "        )\n",
    "    \n",
    "    def chunk(self, extraction_result: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Chunk processed document text using LangChain splitter.\n",
    "        \n",
    "        Args:\n",
    "            extraction_result: Output from TextProcessor.process() or TextExtractor.extract_from_pdf()\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk dictionaries with text and metadata\n",
    "        \"\"\"\n",
    "        if extraction_result['status'] != 'success':\n",
    "            self.logger.warning(f\"Skipping chunking for failed document: {extraction_result['doc_id']}\")\n",
    "            return []\n",
    "        \n",
    "        self.logger.info(f\"Chunking document: {extraction_result['doc_id']}\")\n",
    "        \n",
    "        try:\n",
    "            # Split text into chunks\n",
    "            text = extraction_result['full_text']\n",
    "            chunks = self.text_splitter.split_text(text)\n",
    "            \n",
    "            # Create chunk objects with metadata\n",
    "            chunk_objects = []\n",
    "            for idx, chunk_text in enumerate(chunks):\n",
    "                chunk_obj = {\n",
    "                    'chunk_id': f\"{extraction_result['doc_id']}_chunk_{idx}\",\n",
    "                    'doc_id': extraction_result['doc_id'],\n",
    "                    'doc_name': extraction_result['doc_name'],\n",
    "                    'chunk_index': idx,\n",
    "                    'text': chunk_text,\n",
    "                    'char_count': len(chunk_text),\n",
    "                    'page_numbers': self._estimate_page_numbers(\n",
    "                        chunk_text, \n",
    "                        extraction_result.get('pages', [])\n",
    "                    ),\n",
    "                    'processing_timestamp': self.config.PROCESSING_TIMESTAMP,\n",
    "                    'was_processed': extraction_result.get('processed', False)\n",
    "                }\n",
    "                chunk_objects.append(chunk_obj)\n",
    "            \n",
    "            self.logger.info(f\"‚úì Created {len(chunk_objects)} chunks\")\n",
    "            return chunk_objects\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚úó Chunking failed: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _estimate_page_numbers(self, chunk_text: str, pages: List[Dict]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Estimate which pages a chunk spans based on text matching.\n",
    "        \n",
    "        NOTE: This is a simple heuristic. For production, consider:\n",
    "        - Character position tracking during extraction\n",
    "        - More sophisticated text matching algorithms\n",
    "        - Handling post-processing effects on page boundaries\n",
    "        \"\"\"\n",
    "        if not pages:\n",
    "            return []\n",
    "        \n",
    "        # Find pages containing any portion of the chunk text\n",
    "        chunk_snippet = chunk_text[:100]  # First 100 chars for matching\n",
    "        matching_pages = []\n",
    "        \n",
    "        for page in pages:\n",
    "            if chunk_snippet in page['text']:\n",
    "                matching_pages.append(page['page_number'])\n",
    "        \n",
    "        return matching_pages if matching_pages else [1]  # Default to page 1\n",
    "    \n",
    "    def chunk_batch(self, extraction_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Chunk multiple documents.\n",
    "        \n",
    "        Args:\n",
    "            extraction_results: List of extraction/processing result dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            List of all chunks from all documents\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Chunking batch of {len(extraction_results)} documents\")\n",
    "        \n",
    "        all_chunks = []\n",
    "        for extraction in extraction_results:\n",
    "            chunks = self.chunk(extraction)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        self.logger.info(f\"‚úì Chunking complete: {len(all_chunks)} total chunks created\")\n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "print(\"‚úì TextChunker class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Document Discovery & Validation\n",
    "\n",
    "Locate and validate the 6 sample PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your 6 PDF documents\n",
    "# Modify these paths based on your actual document locations\n",
    "\n",
    "if ProcessingConfig.PDF_SOURCE_TYPE == \"s3\":\n",
    "    # S3 sources\n",
    "    pdf_sources = [\n",
    "        (f\"s3://{ProcessingConfig.S3_BUCKET}/{ProcessingConfig.S3_PREFIX}doc1.pdf\", \"doc_001\"),\n",
    "        (f\"s3://{ProcessingConfig.S3_BUCKET}/{ProcessingConfig.S3_PREFIX}doc2.pdf\", \"doc_002\"),\n",
    "        (f\"s3://{ProcessingConfig.S3_BUCKET}/{ProcessingConfig.S3_PREFIX}doc3.pdf\", \"doc_003\"),\n",
    "        (f\"s3://{ProcessingConfig.S3_BUCKET}/{ProcessingConfig.S3_PREFIX}doc4.pdf\", \"doc_004\"),\n",
    "        (f\"s3://{ProcessingConfig.S3_BUCKET}/{ProcessingConfig.S3_PREFIX}doc5.pdf\", \"doc_005\"),\n",
    "        (f\"s3://{ProcessingConfig.S3_BUCKET}/{ProcessingConfig.S3_PREFIX}doc6.pdf\", \"doc_006\"),\n",
    "    ]\n",
    "else:\n",
    "    # Local sources\n",
    "    pdf_dir = Path(ProcessingConfig.LOCAL_PDF_DIR)\n",
    "    pdf_files = sorted(pdf_dir.glob(\"*.pdf\"))[:6]  # Take first 6 PDFs\n",
    "    pdf_sources = [\n",
    "        (str(pdf_path), f\"doc_{i:03d}\") \n",
    "        for i, pdf_path in enumerate(pdf_files, start=1)\n",
    "    ]\n",
    "\n",
    "# Display document list\n",
    "doc_list_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Doc ID\": doc_id,\n",
    "        \"Source Path\": path,\n",
    "        \"Filename\": Path(path).name\n",
    "    }\n",
    "    for path, doc_id in pdf_sources\n",
    "])\n",
    "\n",
    "print(f\"\\nüìÑ Found {len(pdf_sources)} documents:\")\n",
    "display(doc_list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation: Check if documents are accessible\n",
    "print(\"\\nüîç Validating document access...\\n\")\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "for pdf_path, doc_id in pdf_sources:\n",
    "    try:\n",
    "        if pdf_path.startswith('s3://'):\n",
    "            # Check S3 object exists\n",
    "            parts = pdf_path.replace('s3://', '').split('/', 1)\n",
    "            bucket, key = parts[0], parts[1]\n",
    "            response = s3_client.head_object(Bucket=bucket, Key=key)\n",
    "            size_mb = response['ContentLength'] / (1024 * 1024)\n",
    "            status = \"‚úì Accessible\"\n",
    "        else:\n",
    "            # Check local file exists\n",
    "            path = Path(pdf_path)\n",
    "            if path.exists():\n",
    "                size_mb = path.stat().st_size / (1024 * 1024)\n",
    "                status = \"‚úì Accessible\"\n",
    "            else:\n",
    "                size_mb = 0\n",
    "                status = \"‚úó Not found\"\n",
    "        \n",
    "        validation_results.append({\n",
    "            \"Doc ID\": doc_id,\n",
    "            \"Filename\": Path(pdf_path).name,\n",
    "            \"Size (MB)\": f\"{size_mb:.2f}\",\n",
    "            \"Status\": status\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        validation_results.append({\n",
    "            \"Doc ID\": doc_id,\n",
    "            \"Filename\": Path(pdf_path).name,\n",
    "            \"Size (MB)\": \"N/A\",\n",
    "            \"Status\": f\"‚úó Error: {str(e)[:50]}\"\n",
    "        })\n",
    "\n",
    "validation_df = pd.DataFrame(validation_results)\n",
    "display(validation_df)\n",
    "\n",
    "# Check if all documents are accessible\n",
    "accessible_count = sum(1 for r in validation_results if \"‚úì\" in r[\"Status\"])\n",
    "if accessible_count == len(pdf_sources):\n",
    "    print(f\"\\n‚úì All {len(pdf_sources)} documents are accessible and ready for processing\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: Only {accessible_count}/{len(pdf_sources)} documents are accessible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Raw Extraction (Textractor)\n",
    "\n",
    "Extract text from all PDFs using amazon-textract-textractor and save checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "extractor = TextExtractor(ProcessingConfig)\n",
    "processor = TextProcessor(ProcessingConfig)\n",
    "chunker = TextChunker(ProcessingConfig)\n",
    "\n",
    "print(\"‚úì Processing pipeline initialized\")\n",
    "print(f\"  - TextExtractor: Ready (using Textractor library)\")\n",
    "print(f\"  - TextProcessor: Ready\")\n",
    "print(f\"  - TextChunker: Ready\")\n",
    "print(f\"\\n  Configuration:\")\n",
    "print(f\"  - Chunk size: {ProcessingConfig.CHUNK_SIZE}\")\n",
    "print(f\"  - Chunk overlap: {ProcessingConfig.CHUNK_OVERLAP}\")\n",
    "print(f\"  - Textract features: {ProcessingConfig.TEXTRACT_FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from all documents\n",
    "print(\"\\nüîÑ Starting extraction process...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "raw_extractions = []\n",
    "\n",
    "for pdf_path, doc_id in pdf_sources:\n",
    "    print(f\"\\nProcessing: {doc_id} - {Path(pdf_path).name}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    extraction_result = extractor.extract_from_pdf(pdf_path, doc_id)\n",
    "    raw_extractions.append(extraction_result)\n",
    "    \n",
    "    # Display result summary\n",
    "    if extraction_result['status'] == 'success':\n",
    "        print(f\"  ‚úì Pages: {extraction_result['page_count']}\")\n",
    "        print(f\"  ‚úì Characters: {extraction_result['total_char_count']:,}\")\n",
    "        print(f\"  ‚úì Tables: {extraction_result['table_count']}\")\n",
    "        \n",
    "        # Show layout element counts\n",
    "        total_layout = sum(\n",
    "            sum(page['layout_elements'].values()) \n",
    "            for page in extraction_result['pages']\n",
    "            if page['layout_elements']\n",
    "        )\n",
    "        if total_layout > 0:\n",
    "            print(f\"  ‚úì Layout elements detected: {total_layout}\")\n",
    "    else:\n",
    "        print(f\"  ‚úó Error: {extraction_result.get('error', 'Unknown error')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n‚úì Extraction phase complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw extractions checkpoint\n",
    "# Note: We don't save the textractor_document object (not JSON serializable)\n",
    "extraction_checkpoint_path = checkpoint_dir / ProcessingConfig.RAW_EXTRACTION_FILE\n",
    "\n",
    "# Create a copy without the textractor_document for serialization\n",
    "serializable_extractions = []\n",
    "for extraction in raw_extractions:\n",
    "    extraction_copy = extraction.copy()\n",
    "    if 'textractor_document' in extraction_copy:\n",
    "        del extraction_copy['textractor_document']\n",
    "    serializable_extractions.append(extraction_copy)\n",
    "\n",
    "with open(extraction_checkpoint_path, 'w') as f:\n",
    "    json.dump(serializable_extractions, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Raw extractions saved to: {extraction_checkpoint_path}\")\n",
    "print(f\"   File size: {extraction_checkpoint_path.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Extraction Quality Review\n",
    "\n",
    "Inspect extracted text and layout information for quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "extraction_stats = []\n",
    "\n",
    "for extraction in raw_extractions:\n",
    "    if extraction['status'] == 'success':\n",
    "        # Calculate layout element totals\n",
    "        total_layout = sum(\n",
    "            sum(page['layout_elements'].values()) \n",
    "            for page in extraction['pages']\n",
    "            if page['layout_elements']\n",
    "        )\n",
    "        \n",
    "        extraction_stats.append({\n",
    "            \"Doc ID\": extraction['doc_id'],\n",
    "            \"Doc Name\": extraction['doc_name'],\n",
    "            \"Pages\": extraction['page_count'],\n",
    "            \"Characters\": f\"{extraction['total_char_count']:,}\",\n",
    "            \"Avg Chars/Page\": f\"{extraction['total_char_count'] // extraction['page_count']:,}\",\n",
    "            \"Tables\": extraction['table_count'],\n",
    "            \"Layout Elements\": total_layout,\n",
    "            \"Status\": \"‚úì Success\"\n",
    "        })\n",
    "    else:\n",
    "        extraction_stats.append({\n",
    "            \"Doc ID\": extraction['doc_id'],\n",
    "            \"Doc Name\": extraction['doc_name'],\n",
    "            \"Pages\": \"N/A\",\n",
    "            \"Characters\": \"N/A\",\n",
    "            \"Avg Chars/Page\": \"N/A\",\n",
    "            \"Tables\": \"N/A\",\n",
    "            \"Layout Elements\": \"N/A\",\n",
    "            \"Status\": \"‚úó Failed\"\n",
    "        })\n",
    "\n",
    "stats_df = pd.DataFrame(extraction_stats)\n",
    "print(\"\\nüìä Extraction Statistics:\\n\")\n",
    "display(stats_df)\n",
    "\n",
    "# Overall stats\n",
    "successful = [e for e in raw_extractions if e['status'] == 'success']\n",
    "if successful:\n",
    "    total_pages = sum(e['page_count'] for e in successful)\n",
    "    total_chars = sum(e['total_char_count'] for e in successful)\n",
    "    total_tables = sum(e['table_count'] for e in successful)\n",
    "    \n",
    "    print(f\"\\nüìà Overall:\")\n",
    "    print(f\"   Total pages extracted: {total_pages}\")\n",
    "    print(f\"   Total characters: {total_chars:,}\")\n",
    "    print(f\"   Total tables: {total_tables}\")\n",
    "    print(f\"   Average document size: {total_chars // len(successful):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample text from each document\n",
    "print(\"\\nüìù Sample Text from Each Document (Linearized by Textractor):\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for extraction in raw_extractions:\n",
    "    if extraction['status'] == 'success':\n",
    "        print(f\"\\n{extraction['doc_id']} - {extraction['doc_name']}\")\n",
    "        print(\"-\" * 80)\n",
    "        # Show first 500 characters\n",
    "        sample_text = extraction['full_text'][:500]\n",
    "        print(sample_text)\n",
    "        if len(extraction['full_text']) > 500:\n",
    "            print(\"\\n[... truncated ...]\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display layout information\n",
    "print(\"\\nüèóÔ∏è Layout Analysis:\\n\")\n",
    "\n",
    "for extraction in raw_extractions:\n",
    "    if extraction['status'] == 'success':\n",
    "        print(f\"\\n{extraction['doc_id']} - Layout Element Breakdown:\")\n",
    "        \n",
    "        # Aggregate layout elements across all pages\n",
    "        aggregated_layout = {}\n",
    "        for page in extraction['pages']:\n",
    "            if page['layout_elements']:\n",
    "                for element_type, count in page['layout_elements'].items():\n",
    "                    aggregated_layout[element_type] = aggregated_layout.get(element_type, 0) + count\n",
    "        \n",
    "        if aggregated_layout:\n",
    "            for element_type, count in aggregated_layout.items():\n",
    "                print(f\"  {element_type.capitalize()}: {count}\")\n",
    "        else:\n",
    "            print(\"  No layout elements detected\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality checks\n",
    "print(\"\\nüîç Quality Checks:\\n\")\n",
    "\n",
    "quality_issues = []\n",
    "\n",
    "for extraction in raw_extractions:\n",
    "    if extraction['status'] == 'success':\n",
    "        text = extraction['full_text']\n",
    "        doc_id = extraction['doc_id']\n",
    "        \n",
    "        # Check for potential issues\n",
    "        issues = []\n",
    "        \n",
    "        # Very short extraction\n",
    "        if len(text) < 100:\n",
    "            issues.append(\"Very short text (< 100 chars)\")\n",
    "        \n",
    "        # Check for excessive special characters (possible OCR issues)\n",
    "        special_char_ratio = sum(1 for c in text if not c.isalnum() and not c.isspace()) / len(text)\n",
    "        if special_char_ratio > 0.3:\n",
    "            issues.append(f\"High special char ratio ({special_char_ratio:.1%})\")\n",
    "        \n",
    "        # Check for repeated characters (OCR artifact)\n",
    "        if '.....' in text or '-----' in text:\n",
    "            issues.append(\"Repeated characters detected\")\n",
    "        \n",
    "        if issues:\n",
    "            quality_issues.append({\n",
    "                \"Doc ID\": doc_id,\n",
    "                \"Issues\": \"; \".join(issues)\n",
    "            })\n",
    "\n",
    "if quality_issues:\n",
    "    quality_df = pd.DataFrame(quality_issues)\n",
    "    print(\"‚ö†Ô∏è Potential Quality Issues Detected:\\n\")\n",
    "    display(quality_df)\n",
    "else:\n",
    "    print(\"‚úì No obvious quality issues detected\")\n",
    "    print(\"   Note: Manual review of sample text above is still recommended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.5 Text Post-Processing\n",
    "\n",
    "Apply cleaning and normalization to extracted text before chunking.\n",
    "\n",
    "**Processing Steps:**\n",
    "1. Clean whitespace\n",
    "2. Fix hyphenation (words split across lines)\n",
    "3. Remove headers/footers\n",
    "4. Normalize unicode characters\n",
    "5. Remove page numbers\n",
    "\n",
    "**Custom processing:** Modify `TextProcessor` class methods to add domain-specific logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all successfully extracted documents\n",
    "print(\"\\nüîÑ Starting text processing...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "processed_extractions = []\n",
    "\n",
    "for extraction in raw_extractions:\n",
    "    if extraction['status'] == 'success':\n",
    "        print(f\"\\nProcessing: {extraction['doc_id']} - {extraction['doc_name']}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        original_length = extraction['total_char_count']\n",
    "        \n",
    "        processed = processor.process(extraction)\n",
    "        processed_extractions.append(processed)\n",
    "        \n",
    "        if processed.get('processed', False):\n",
    "            new_length = processed['char_count_after_processing']\n",
    "            chars_removed = processed['chars_removed']\n",
    "            print(f\"  ‚úì Original: {original_length:,} chars\")\n",
    "            print(f\"  ‚úì Processed: {new_length:,} chars\")\n",
    "            print(f\"  ‚úì Removed: {chars_removed:,} chars ({chars_removed/original_length*100:.1f}%)\")\n",
    "            print(f\"  ‚úì Steps: {', '.join(processed['processing_steps'])}\")\n",
    "        else:\n",
    "            print(f\"  ‚úó Processing failed\")\n",
    "    else:\n",
    "        # Keep failed extractions in the list\n",
    "        processed_extractions.append(extraction)\n",
    "        print(f\"\\nSkipping: {extraction['doc_id']} (extraction failed)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n‚úì Text processing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed extractions checkpoint\n",
    "processed_checkpoint_path = checkpoint_dir / ProcessingConfig.PROCESSED_EXTRACTIONS_FILE\n",
    "\n",
    "# Create serializable version (remove textractor_document if present)\n",
    "serializable_processed = []\n",
    "for extraction in processed_extractions:\n",
    "    extraction_copy = extraction.copy()\n",
    "    if 'textractor_document' in extraction_copy:\n",
    "        del extraction_copy['textractor_document']\n",
    "    serializable_processed.append(extraction_copy)\n",
    "\n",
    "with open(processed_checkpoint_path, 'w') as f:\n",
    "    json.dump(serializable_processed, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Processed extractions saved to: {processed_checkpoint_path}\")\n",
    "print(f\"   File size: {processed_checkpoint_path.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare before/after statistics\n",
    "print(\"\\nüìä Processing Impact Analysis:\\n\")\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for i, extraction in enumerate(raw_extractions):\n",
    "    if extraction['status'] == 'success':\n",
    "        processed = processed_extractions[i]\n",
    "        if processed.get('processed', False):\n",
    "            original = extraction['total_char_count']\n",
    "            after = processed['char_count_after_processing']\n",
    "            removed = processed['chars_removed']\n",
    "            \n",
    "            comparison_data.append({\n",
    "                \"Doc ID\": extraction['doc_id'],\n",
    "                \"Original Chars\": f\"{original:,}\",\n",
    "                \"Processed Chars\": f\"{after:,}\",\n",
    "                \"Chars Removed\": f\"{removed:,}\",\n",
    "                \"Reduction %\": f\"{removed/original*100:.1f}%\"\n",
    "            })\n",
    "\n",
    "if comparison_data:\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_original = sum(int(r[\"Original Chars\"].replace(',', '')) for r in comparison_data)\n",
    "    total_processed = sum(int(r[\"Processed Chars\"].replace(',', '')) for r in comparison_data)\n",
    "    total_removed = total_original - total_processed\n",
    "    \n",
    "    print(f\"\\nüìà Overall Impact:\")\n",
    "    print(f\"   Total original: {total_original:,} characters\")\n",
    "    print(f\"   Total processed: {total_processed:,} characters\")\n",
    "    print(f\"   Total removed: {total_removed:,} characters ({total_removed/total_original*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display before/after samples\n",
    "print(\"\\nüìù Before/After Text Samples:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show first document as example\n",
    "for i, extraction in enumerate(raw_extractions[:2]):  # First 2 docs\n",
    "    if extraction['status'] == 'success':\n",
    "        processed = processed_extractions[i]\n",
    "        if processed.get('processed', False):\n",
    "            print(f\"\\n{extraction['doc_id']} - {extraction['doc_name']}\")\n",
    "            print(\"-\" * 80)\n",
    "            print(\"\\nBEFORE (first 300 chars):\")\n",
    "            print(extraction['full_text'][:300])\n",
    "            print(\"\\nAFTER (first 300 chars):\")\n",
    "            print(processed['full_text'][:300])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Text Chunking\n",
    "\n",
    "Apply LangChain RecursiveCharacterTextSplitter to create chunks from processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk all successfully processed documents\n",
    "print(\"\\nüîÑ Starting chunking process...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "for extraction in processed_extractions:\n",
    "    if extraction['status'] == 'success':\n",
    "        print(f\"\\nChunking: {extraction['doc_id']} - {extraction['doc_name']}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        chunks = chunker.chunk(extraction)\n",
    "        all_chunks.extend(chunks)\n",
    "        \n",
    "        print(f\"  ‚úì Created {len(chunks)} chunks\")\n",
    "        if chunks:\n",
    "            avg_chunk_size = sum(c['char_count'] for c in chunks) / len(chunks)\n",
    "            print(f\"  ‚úì Average chunk size: {avg_chunk_size:.0f} characters\")\n",
    "            print(f\"  ‚úì Used processed text: {chunks[0]['was_processed']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\n‚úì Chunking complete: {len(all_chunks)} total chunks created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Chunk Analysis\n",
    "\n",
    "Analyze chunk distribution and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk distribution by document\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if all_chunks:\n",
    "    # Group chunks by document\n",
    "    chunks_by_doc = {}\n",
    "    for chunk in all_chunks:\n",
    "        doc_id = chunk['doc_id']\n",
    "        if doc_id not in chunks_by_doc:\n",
    "            chunks_by_doc[doc_id] = []\n",
    "        chunks_by_doc[doc_id].append(chunk)\n",
    "    \n",
    "    # Create distribution table\n",
    "    distribution_data = []\n",
    "    for doc_id, chunks in chunks_by_doc.items():\n",
    "        chunk_sizes = [c['char_count'] for c in chunks]\n",
    "        distribution_data.append({\n",
    "            \"Doc ID\": doc_id,\n",
    "            \"Chunk Count\": len(chunks),\n",
    "            \"Avg Size\": f\"{sum(chunk_sizes) / len(chunks):.0f}\",\n",
    "            \"Min Size\": min(chunk_sizes),\n",
    "            \"Max Size\": max(chunk_sizes),\n",
    "            \"Total Chars\": f\"{sum(chunk_sizes):,}\"\n",
    "        })\n",
    "    \n",
    "    dist_df = pd.DataFrame(distribution_data)\n",
    "    print(\"\\nüìä Chunk Distribution by Document:\\n\")\n",
    "    display(dist_df)\n",
    "    \n",
    "    # Visualize chunk size distribution\n",
    "    all_chunk_sizes = [c['char_count'] for c in all_chunks]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(all_chunk_sizes, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.axvline(ProcessingConfig.CHUNK_SIZE, color='red', linestyle='--', label=f'Target: {ProcessingConfig.CHUNK_SIZE}')\n",
    "    plt.xlabel('Chunk Size (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Chunk Size Distribution')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    doc_ids = [d[\"Doc ID\"] for d in distribution_data]\n",
    "    chunk_counts = [d[\"Chunk Count\"] for d in distribution_data]\n",
    "    plt.bar(doc_ids, chunk_counts, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Document ID')\n",
    "    plt.ylabel('Number of Chunks')\n",
    "    plt.title('Chunks per Document')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(checkpoint_dir / 'chunk_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìà Visualization saved to: {checkpoint_dir / 'chunk_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example chunks\n",
    "print(\"\\nüìù Example Chunks:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show first chunk from each document\n",
    "if all_chunks:\n",
    "    docs_shown = set()\n",
    "    for chunk in all_chunks:\n",
    "        if chunk['doc_id'] not in docs_shown:\n",
    "            docs_shown.add(chunk['doc_id'])\n",
    "            print(f\"\\n{chunk['chunk_id']}\")\n",
    "            print(f\"Doc: {chunk['doc_name']}, Pages: {chunk['page_numbers']}, Size: {chunk['char_count']} chars\")\n",
    "            print(\"-\" * 80)\n",
    "            # Show first 300 characters\n",
    "            print(chunk['text'][:300])\n",
    "            if len(chunk['text']) > 300:\n",
    "                print(\"\\n[... truncated ...]\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk boundary analysis\n",
    "print(\"\\nüîç Chunk Boundary Analysis:\\n\")\n",
    "\n",
    "if all_chunks:\n",
    "    # Check for very small chunks (might indicate issues)\n",
    "    small_chunks = [c for c in all_chunks if c['char_count'] < ProcessingConfig.CHUNK_SIZE * 0.3]\n",
    "    \n",
    "    # Check for chunks at max size (might be cut mid-sentence)\n",
    "    large_chunks = [c for c in all_chunks if c['char_count'] >= ProcessingConfig.CHUNK_SIZE * 0.95]\n",
    "    \n",
    "    print(f\"Small chunks (< 30% of target): {len(small_chunks)} / {len(all_chunks)} ({len(small_chunks)/len(all_chunks)*100:.1f}%)\")\n",
    "    print(f\"Large chunks (>= 95% of target): {len(large_chunks)} / {len(all_chunks)} ({len(large_chunks)/len(all_chunks)*100:.1f}%)\")\n",
    "    \n",
    "    # Check overlap effectiveness\n",
    "    print(f\"\\nOverlap check: Comparing consecutive chunks...\")\n",
    "    overlap_examples = 0\n",
    "    for i in range(min(3, len(all_chunks) - 1)):\n",
    "        chunk1 = all_chunks[i]\n",
    "        chunk2 = all_chunks[i + 1]\n",
    "        if chunk1['doc_id'] == chunk2['doc_id']:  # Same document\n",
    "            # Check if there's any overlap\n",
    "            end_of_first = chunk1['text'][-100:]  # Last 100 chars of first chunk\n",
    "            start_of_second = chunk2['text'][:100]  # First 100 chars of second chunk\n",
    "            \n",
    "            # Simple overlap detection\n",
    "            overlap_found = any(end_of_first[i:i+20] in start_of_second for i in range(len(end_of_first)-20))\n",
    "            \n",
    "            if overlap_found:\n",
    "                overlap_examples += 1\n",
    "    \n",
    "    print(f\"  Overlap detected in {overlap_examples}/3 sample consecutive chunk pairs\")\n",
    "    print(f\"  (Expected behavior with overlap={ProcessingConfig.CHUNK_OVERLAP})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Final Output Preparation\n",
    "\n",
    "Validate and save processed chunks for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema validation\n",
    "print(\"\\n‚úÖ Schema Validation:\\n\")\n",
    "\n",
    "required_fields = ['chunk_id', 'doc_id', 'doc_name', 'text', 'chunk_index', 'char_count', 'page_numbers']\n",
    "\n",
    "validation_passed = True\n",
    "for chunk in all_chunks[:5]:  # Check first 5 chunks\n",
    "    missing_fields = [field for field in required_fields if field not in chunk]\n",
    "    if missing_fields:\n",
    "        print(f\"‚úó Chunk {chunk.get('chunk_id', 'unknown')} missing fields: {missing_fields}\")\n",
    "        validation_passed = False\n",
    "\n",
    "if validation_passed:\n",
    "    print(\"‚úì All chunks have required fields\")\n",
    "    print(f\"‚úì Schema: {', '.join(required_fields)}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Warning: Some chunks have missing fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final output format\n",
    "# Add any additional metadata or formatting needed for OpenSearch\n",
    "\n",
    "final_chunks = []\n",
    "for chunk in all_chunks:\n",
    "    # Create a clean version for indexing\n",
    "    final_chunk = {\n",
    "        'id': chunk['chunk_id'],  # Use as document ID in OpenSearch\n",
    "        'text': chunk['text'],\n",
    "        'metadata': {\n",
    "            'doc_id': chunk['doc_id'],\n",
    "            'doc_name': chunk['doc_name'],\n",
    "            'chunk_index': chunk['chunk_index'],\n",
    "            'char_count': chunk['char_count'],\n",
    "            'page_numbers': chunk['page_numbers'],\n",
    "            'processing_timestamp': chunk['processing_timestamp'],\n",
    "            'was_processed': chunk['was_processed']\n",
    "        }\n",
    "    }\n",
    "    final_chunks.append(final_chunk)\n",
    "\n",
    "print(f\"‚úì Prepared {len(final_chunks)} chunks for indexing\")\n",
    "print(f\"\\nSample output format:\")\n",
    "print(json.dumps(final_chunks[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed chunks\n",
    "chunks_output_path = checkpoint_dir / ProcessingConfig.PROCESSED_CHUNKS_FILE\n",
    "\n",
    "with open(chunks_output_path, 'w') as f:\n",
    "    json.dump(final_chunks, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Processed chunks saved to: {chunks_output_path}\")\n",
    "print(f\"   File size: {chunks_output_path.stat().st_size / 1024:.2f} KB\")\n",
    "print(f\"   Total chunks: {len(final_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã PROCESSING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "successful_extractions = [e for e in raw_extractions if e['status'] == 'success']\n",
    "successful_processing = [e for e in processed_extractions if e.get('processed', False)]\n",
    "\n",
    "print(f\"\\nüìÑ Document Processing:\")\n",
    "print(f\"   Total documents: {len(pdf_sources)}\")\n",
    "print(f\"   Successfully extracted: {len(successful_extractions)}\")\n",
    "print(f\"   Successfully processed: {len(successful_processing)}\")\n",
    "print(f\"   Failed extractions: {len(pdf_sources) - len(successful_extractions)}\")\n",
    "\n",
    "if successful_extractions:\n",
    "    total_pages = sum(e['page_count'] for e in successful_extractions)\n",
    "    total_chars_raw = sum(e['total_char_count'] for e in successful_extractions)\n",
    "    total_tables = sum(e['table_count'] for e in successful_extractions)\n",
    "    print(f\"   Total pages: {total_pages}\")\n",
    "    print(f\"   Total characters (raw): {total_chars_raw:,}\")\n",
    "    print(f\"   Total tables detected: {total_tables}\")\n",
    "\n",
    "if successful_processing:\n",
    "    total_chars_processed = sum(e['char_count_after_processing'] for e in successful_processing)\n",
    "    total_removed = total_chars_raw - total_chars_processed\n",
    "    print(f\"   Total characters (processed): {total_chars_processed:,}\")\n",
    "    print(f\"   Characters removed by processing: {total_removed:,} ({total_removed/total_chars_raw*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüì¶ Chunk Generation:\")\n",
    "print(f\"   Total chunks created: {len(all_chunks)}\")\n",
    "if all_chunks:\n",
    "    avg_chunk_size = sum(c['char_count'] for c in all_chunks) / len(all_chunks)\n",
    "    chunks_from_processed = sum(1 for c in all_chunks if c.get('was_processed', False))\n",
    "    print(f\"   Average chunk size: {avg_chunk_size:.0f} characters\")\n",
    "    print(f\"   Target chunk size: {ProcessingConfig.CHUNK_SIZE} characters\")\n",
    "    print(f\"   Chunk overlap: {ProcessingConfig.CHUNK_OVERLAP} characters\")\n",
    "    print(f\"   Chunks from processed text: {chunks_from_processed}/{len(all_chunks)}\")\n",
    "\n",
    "print(f\"\\nüíæ Outputs:\")\n",
    "print(f\"   Config: {checkpoint_dir / 'config.json'}\")\n",
    "print(f\"   Raw extractions: {checkpoint_dir / ProcessingConfig.RAW_EXTRACTION_FILE}\")\n",
    "print(f\"   Processed extractions: {checkpoint_dir / ProcessingConfig.PROCESSED_EXTRACTIONS_FILE}\")\n",
    "print(f\"   Final chunks: {checkpoint_dir / ProcessingConfig.PROCESSED_CHUNKS_FILE}\")\n",
    "print(f\"   Visualization: {checkpoint_dir / 'chunk_distribution.png'}\")\n",
    "\n",
    "print(f\"\\nüîß Pipeline Components Used:\")\n",
    "print(f\"   1. TextExtractor (amazon-textract-textractor) ‚Üí Layout-aware text extraction\")\n",
    "print(f\"   2. TextProcessor ‚Üí Cleaned and normalized text\")\n",
    "print(f\"   3. TextChunker (LangChain) ‚Üí Final chunks for indexing\")\n",
    "\n",
    "print(f\"\\n‚è≠Ô∏è  Next Steps:\")\n",
    "print(f\"   1. Review the layout analysis in Section 6\")\n",
    "print(f\"   2. Review the before/after samples in Section 6.5\")\n",
    "print(f\"   3. Review the chunk samples in Section 8\")\n",
    "print(f\"   4. If satisfied, proceed to: 02_indexing.ipynb\")\n",
    "print(f\"   5. If adjustments needed:\")\n",
    "print(f\"      - Modify TextProcessor methods for different cleaning\")\n",
    "print(f\"      - Modify config and re-run from Section 6.5 (load raw checkpoint)\")\n",
    "print(f\"      - Modify chunking params and re-run from Section 7 (load processed checkpoint)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì Notebook execution complete\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notes & Observations\n",
    "\n",
    "**Architecture:**\n",
    "- Three-class design: `TextExtractor`, `TextProcessor`, `TextChunker`\n",
    "- Using amazon-textract-textractor library for layout-aware extraction\n",
    "- Separation of concerns allows independent testing and iteration\n",
    "- Each stage has its own checkpoint for efficient experimentation\n",
    "\n",
    "**Extraction Quality:**\n",
    "- [Add observations about Textractor layout detection]\n",
    "- [Note quality of linearized text vs raw text]\n",
    "- [Document any problematic documents]\n",
    "\n",
    "**Layout Detection:**\n",
    "- [Note which layout elements were detected (titles, headers, tables, etc.)]\n",
    "- [Assess if layout-aware extraction improved text quality]\n",
    "- [Consider using layout info for chunking in future]\n",
    "\n",
    "**Text Processing:**\n",
    "- [Document effectiveness of cleaning steps]\n",
    "- [Note any additional processing needed]\n",
    "- [List custom methods added to TextProcessor]\n",
    "\n",
    "**Chunking Strategy:**\n",
    "- [Document why you chose these chunk parameters]\n",
    "- [Note any adjustments made during experimentation]\n",
    "- [Consider layout-aware chunking for next iteration]\n",
    "\n",
    "**Issues Encountered:**\n",
    "- [List any errors or unexpected behavior]\n",
    "\n",
    "**Next Experiments:**\n",
    "- [Try different processing strategies]\n",
    "- [Alternative chunk sizes or overlap values]\n",
    "- [Layout-based chunking (chunk by sections instead of character count)]\n",
    "- [Use table data for structured retrieval]\n",
    "- [Metadata enhancements from layout elements]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
