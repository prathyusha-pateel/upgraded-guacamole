{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Processing Pipeline\n",
    "## Extract and Chunk PDFs using Textractor and LangChain\n",
    "\n",
    "**Purpose**: Process 6 sample PDF documents through Textractor extraction and LangChain recursive splitting to prepare for BM25 indexing.\n",
    "\n",
    "**Outputs**: \n",
    "- Raw extracted text (saved as checkpoint)\n",
    "- Processed chunks with metadata (ready for indexing)\n",
    "\n",
    "**Next Step**: `02_indexing.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "from textractcaller import call_textract\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Logging setup\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úì Imports completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AWS clients\n",
    "session = boto3.Session()\n",
    "s3_client = session.client('s3')\n",
    "textract_client = session.client('textract')\n",
    "\n",
    "# Verify credentials\n",
    "sts = session.client('sts')\n",
    "identity = sts.get_caller_identity()\n",
    "print(f\"‚úì AWS Identity: {identity['Arn']}\")\n",
    "print(f\"‚úì Region: {session.region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration\n",
    "\n",
    "Centralize all parameters for reproducibility and easy experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessingConfig:\n",
    "    \"\"\"Configuration for document processing pipeline.\"\"\"\n",
    "    \n",
    "    # Document sources\n",
    "    PDF_SOURCE_TYPE = \"s3\"  # or \"local\"\n",
    "    S3_BUCKET = \"your-bucket-name\"\n",
    "    S3_PREFIX = \"raw-pdfs/\"\n",
    "    LOCAL_PDF_DIR = \"./data/pdfs/\"\n",
    "    \n",
    "    # Output paths\n",
    "    CHECKPOINT_DIR = \"./checkpoints/\"\n",
    "    RAW_EXTRACTION_FILE = \"raw_extractions.json\"\n",
    "    PROCESSED_CHUNKS_FILE = \"processed_chunks.json\"\n",
    "    \n",
    "    # Textractor settings\n",
    "    TEXTRACT_FEATURES = []  # Empty for text only, or [\"TABLES\", \"FORMS\"] for structured data\n",
    "    \n",
    "    # Chunking parameters\n",
    "    CHUNK_SIZE = 1000\n",
    "    CHUNK_OVERLAP = 200\n",
    "    SEPARATORS = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    LENGTH_FUNCTION = len\n",
    "    \n",
    "    # Processing\n",
    "    BATCH_SIZE = 6  # Number of documents\n",
    "    PROCESSING_TIMESTAMP = datetime.utcnow().isoformat()\n",
    "    \n",
    "    @classmethod\n",
    "    def to_dict(cls) -> Dict[str, Any]:\n",
    "        \"\"\"Export config as dictionary for logging.\"\"\"\n",
    "        return {\n",
    "            k: v for k, v in cls.__dict__.items() \n",
    "            if not k.startswith('_') and not callable(v)\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def save(cls, filepath: str):\n",
    "        \"\"\"Save configuration to JSON.\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(cls.to_dict(), f, indent=2)\n",
    "\n",
    "# Display current configuration\n",
    "config_df = pd.DataFrame([\n",
    "    {\"Parameter\": k, \"Value\": v} \n",
    "    for k, v in ProcessingConfig.to_dict().items()\n",
    "])\n",
    "print(\"\\nüìã Current Configuration:\")\n",
    "display(config_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directory\n",
    "checkpoint_dir = Path(ProcessingConfig.CHECKPOINT_DIR)\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save config for reproducibility\n",
    "ProcessingConfig.save(checkpoint_dir / \"config.json\")\n",
    "print(f\"‚úì Configuration saved to {checkpoint_dir / 'config.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. DocumentProcessor Class\n",
    "\n",
    "Production-ready class for extraction and chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"Handles PDF extraction via Textractor and chunking via LangChain.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        self.textract_client = boto3.client('textract')\n",
    "        \n",
    "        # Initialize text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=config.CHUNK_SIZE,\n",
    "            chunk_overlap=config.CHUNK_OVERLAP,\n",
    "            separators=config.SEPARATORS,\n",
    "            length_function=config.LENGTH_FUNCTION,\n",
    "        )\n",
    "        \n",
    "    def extract_from_pdf(self, pdf_path: str, doc_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract text from PDF using Textractor.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: S3 URI (s3://bucket/key) or local file path\n",
    "            doc_id: Unique identifier for this document\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with extracted text and metadata\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Extracting text from: {pdf_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Call Textractor\n",
    "            if pdf_path.startswith('s3://'):\n",
    "                # Parse S3 URI\n",
    "                parts = pdf_path.replace('s3://', '').split('/', 1)\n",
    "                bucket, key = parts[0], parts[1]\n",
    "                \n",
    "                response = call_textract(\n",
    "                    input_document=f\"s3://{bucket}/{key}\",\n",
    "                    features=self.config.TEXTRACT_FEATURES,\n",
    "                    boto3_textract_client=self.textract_client\n",
    "                )\n",
    "            else:\n",
    "                # Local file\n",
    "                with open(pdf_path, 'rb') as f:\n",
    "                    response = call_textract(\n",
    "                        input_document=f.read(),\n",
    "                        features=self.config.TEXTRACT_FEATURES,\n",
    "                        boto3_textract_client=self.textract_client\n",
    "                    )\n",
    "            \n",
    "            # Extract text and page information\n",
    "            pages = []\n",
    "            full_text = \"\"\n",
    "            \n",
    "            for page_num, page in enumerate(response.pages, start=1):\n",
    "                page_text = page.get_text()\n",
    "                pages.append({\n",
    "                    'page_number': page_num,\n",
    "                    'text': page_text,\n",
    "                    'char_count': len(page_text)\n",
    "                })\n",
    "                full_text += page_text + \"\\n\\n\"\n",
    "            \n",
    "            result = {\n",
    "                'doc_id': doc_id,\n",
    "                'doc_name': Path(pdf_path).name,\n",
    "                'source_path': pdf_path,\n",
    "                'full_text': full_text.strip(),\n",
    "                'pages': pages,\n",
    "                'page_count': len(pages),\n",
    "                'total_char_count': len(full_text),\n",
    "                'extraction_timestamp': datetime.utcnow().isoformat(),\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "            self.logger.info(f\"‚úì Extracted {len(pages)} pages, {len(full_text):,} characters\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚úó Extraction failed: {str(e)}\")\n",
    "            return {\n",
    "                'doc_id': doc_id,\n",
    "                'doc_name': Path(pdf_path).name,\n",
    "                'source_path': pdf_path,\n",
    "                'status': 'failed',\n",
    "                'error': str(e),\n",
    "                'extraction_timestamp': datetime.utcnow().isoformat()\n",
    "            }\n",
    "    \n",
    "    def chunk_document(self, extraction_result: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Chunk extracted document text using LangChain splitter.\n",
    "        \n",
    "        Args:\n",
    "            extraction_result: Output from extract_from_pdf()\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk dictionaries with text and metadata\n",
    "        \"\"\"\n",
    "        if extraction_result['status'] != 'success':\n",
    "            self.logger.warning(f\"Skipping chunking for failed extraction: {extraction_result['doc_id']}\")\n",
    "            return []\n",
    "        \n",
    "        self.logger.info(f\"Chunking document: {extraction_result['doc_id']}\")\n",
    "        \n",
    "        try:\n",
    "            # Split text into chunks\n",
    "            text = extraction_result['full_text']\n",
    "            chunks = self.text_splitter.split_text(text)\n",
    "            \n",
    "            # Create chunk objects with metadata\n",
    "            chunk_objects = []\n",
    "            for idx, chunk_text in enumerate(chunks):\n",
    "                chunk_obj = {\n",
    "                    'chunk_id': f\"{extraction_result['doc_id']}_chunk_{idx}\",\n",
    "                    'doc_id': extraction_result['doc_id'],\n",
    "                    'doc_name': extraction_result['doc_name'],\n",
    "                    'chunk_index': idx,\n",
    "                    'text': chunk_text,\n",
    "                    'char_count': len(chunk_text),\n",
    "                    'page_numbers': self._estimate_page_numbers(\n",
    "                        chunk_text, \n",
    "                        extraction_result.get('pages', [])\n",
    "                    ),\n",
    "                    'processing_timestamp': self.config.PROCESSING_TIMESTAMP\n",
    "                }\n",
    "                chunk_objects.append(chunk_obj)\n",
    "            \n",
    "            self.logger.info(f\"‚úì Created {len(chunk_objects)} chunks\")\n",
    "            return chunk_objects\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚úó Chunking failed: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _estimate_page_numbers(self, chunk_text: str, pages: List[Dict]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Estimate which pages a chunk spans based on text matching.\n",
    "        This is a simple heuristic - could be improved.\n",
    "        \"\"\"\n",
    "        if not pages:\n",
    "            return []\n",
    "        \n",
    "        # Find pages containing any portion of the chunk text\n",
    "        chunk_snippet = chunk_text[:100]  # First 100 chars for matching\n",
    "        matching_pages = []\n",
    "        \n",
    "        for page in pages:\n",
    "            if chunk_snippet in page['text']:\n",
    "                matching_pages.append(page['page_number'])\n",
    "        \n",
    "        return matching_pages if matching_pages else [1]  # Default to page 1\n",
    "    \n",
    "    def process_document(self, pdf_path: str, doc_id: str) -> tuple:\n",
    "        \"\"\"\n",
    "        End-to-end processing: extract and chunk.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (extraction_result, chunks)\n",
    "        \"\"\"\n",
    "        extraction_result = self.extract_from_pdf(pdf_path, doc_id)\n",
    "        chunks = self.chunk_document(extraction_result)\n",
    "        return extraction_result, chunks\n",
    "    \n",
    "    def process_batch(self, pdf_sources: List[tuple]) -> tuple:\n",
    "        \"\"\"\n",
    "        Process multiple documents.\n",
    "        \n",
    "        Args:\n",
    "            pdf_sources: List of (pdf_path, doc_id) tuples\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (all_extractions, all_chunks)\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Processing batch of {len(pdf_sources)} documents\")\n",
    "        \n",
    "        all_extractions = []\n",
    "        all_chunks = []\n",
    "        \n",
    "        for pdf_path, doc_id in pdf_sources:\n",
    "            extraction, chunks = self.process_document(pdf_path, doc_id)\n",
    "            all_extractions.append(extraction)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        success_count = sum(1 for e in all_extractions if e['status'] == 'success')\n",
    "        self.logger.info(f\"‚úì Batch complete: {success_count}/{len(pdf_sources)} successful\")\n",
    "        \n",
    "        return all_extractions, all_chunks\n",
    "\n",
    "print(\"‚úì DocumentProcessor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Document Discovery & Validation\n",
    "\n",
    "Locate and validate the 6 sample PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your 6 PDF documents\n",
    "# Modify these paths based on your actual document locations\n",
    "\n",
    "if ProcessingConfig.PDF_SOURCE_TYPE == \"s3\":\n",
    "    # S3 sources\n",
    "    pdf_sources = [\n",
    "        (f\"s3://{ProcessingConfig.S3_BUCKET}/{ProcessingConfig.S3_PREFIX}doc1.pdf\", \"doc_001\"),\n",
    "        (f\"s3://{ProcessingConfig.S3_BUCKET}/{ProcessingConfig.S3_PREFIX}doc2.pdf\", \"doc_002\"),\n",
    "        (f\"s3://{ProcessingConfig.S3_BUCKET}/{ProcessingConfig.S3_PREFIX}doc3.pdf\", \"doc_003\"),\n",
    "        (f\"s3://{ProcessingConfig.S3_BUCKET}/{ProcessingConfig.S3_PREFIX}doc4.pdf\", \"doc_004\"),\n",
    "        (f\"s3://{ProcessingConfig.S3_BUCKET}/{ProcessingConfig.S3_PREFIX}doc5.pdf\", \"doc_005\"),\n",
    "        (f\"s3://{ProcessingConfig.S3_BUCKET}/{ProcessingConfig.S3_PREFIX}doc6.pdf\", \"doc_006\"),\n",
    "    ]\n",
    "else:\n",
    "    # Local sources\n",
    "    pdf_dir = Path(ProcessingConfig.LOCAL_PDF_DIR)\n",
    "    pdf_files = sorted(pdf_dir.glob(\"*.pdf\"))[:6]  # Take first 6 PDFs\n",
    "    pdf_sources = [\n",
    "        (str(pdf_path), f\"doc_{i:03d}\") \n",
    "        for i, pdf_path in enumerate(pdf_files, start=1)\n",
    "    ]\n",
    "\n",
    "# Display document list\n",
    "doc_list_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Doc ID\": doc_id,\n",
    "        \"Source Path\": path,\n",
    "        \"Filename\": Path(path).name\n",
    "    }\n",
    "    for path, doc_id in pdf_sources\n",
    "])\n",
    "\n",
    "print(f\"\\nüìÑ Found {len(pdf_sources)} documents:\")\n",
    "display(doc_list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation: Check if documents are accessible\n",
    "print(\"\\nüîç Validating document access...\\n\")\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "for pdf_path, doc_id in pdf_sources:\n",
    "    try:\n",
    "        if pdf_path.startswith('s3://'):\n",
    "            # Check S3 object exists\n",
    "            parts = pdf_path.replace('s3://', '').split('/', 1)\n",
    "            bucket, key = parts[0], parts[1]\n",
    "            response = s3_client.head_object(Bucket=bucket, Key=key)\n",
    "            size_mb = response['ContentLength'] / (1024 * 1024)\n",
    "            status = \"‚úì Accessible\"\n",
    "        else:\n",
    "            # Check local file exists\n",
    "            path = Path(pdf_path)\n",
    "            if path.exists():\n",
    "                size_mb = path.stat().st_size / (1024 * 1024)\n",
    "                status = \"‚úì Accessible\"\n",
    "            else:\n",
    "                size_mb = 0\n",
    "                status = \"‚úó Not found\"\n",
    "        \n",
    "        validation_results.append({\n",
    "            \"Doc ID\": doc_id,\n",
    "            \"Filename\": Path(pdf_path).name,\n",
    "            \"Size (MB)\": f\"{size_mb:.2f}\",\n",
    "            \"Status\": status\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        validation_results.append({\n",
    "            \"Doc ID\": doc_id,\n",
    "            \"Filename\": Path(pdf_path).name,\n",
    "            \"Size (MB)\": \"N/A\",\n",
    "            \"Status\": f\"‚úó Error: {str(e)[:50]}\"\n",
    "        })\n",
    "\n",
    "validation_df = pd.DataFrame(validation_results)\n",
    "display(validation_df)\n",
    "\n",
    "# Check if all documents are accessible\n",
    "accessible_count = sum(1 for r in validation_results if \"‚úì\" in r[\"Status\"])\n",
    "if accessible_count == len(pdf_sources):\n",
    "    print(f\"\\n‚úì All {len(pdf_sources)} documents are accessible and ready for processing\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: Only {accessible_count}/{len(pdf_sources)} documents are accessible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Raw Extraction (Textractor)\n",
    "\n",
    "Extract text from all PDFs and save checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "processor = DocumentProcessor(ProcessingConfig)\n",
    "\n",
    "print(\"‚úì DocumentProcessor initialized\")\n",
    "print(f\"  - Chunk size: {ProcessingConfig.CHUNK_SIZE}\")\n",
    "print(f\"  - Chunk overlap: {ProcessingConfig.CHUNK_OVERLAP}\")\n",
    "print(f\"  - Textract features: {ProcessingConfig.TEXTRACT_FEATURES or 'Text only'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from all documents\n",
    "print(\"\\nüîÑ Starting extraction process...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "raw_extractions = []\n",
    "\n",
    "for pdf_path, doc_id in pdf_sources:\n",
    "    print(f\"\\nProcessing: {doc_id} - {Path(pdf_path).name}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    extraction_result = processor.extract_from_pdf(pdf_path, doc_id)\n",
    "    raw_extractions.append(extraction_result)\n",
    "    \n",
    "    # Display result summary\n",
    "    if extraction_result['status'] == 'success':\n",
    "        print(f\"  ‚úì Pages: {extraction_result['page_count']}\")\n",
    "        print(f\"  ‚úì Characters: {extraction_result['total_char_count']:,}\")\n",
    "    else:\n",
    "        print(f\"  ‚úó Error: {extraction_result.get('error', 'Unknown error')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n‚úì Extraction phase complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw extractions checkpoint\n",
    "extraction_checkpoint_path = checkpoint_dir / ProcessingConfig.RAW_EXTRACTION_FILE\n",
    "\n",
    "with open(extraction_checkpoint_path, 'w') as f:\n",
    "    json.dump(raw_extractions, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Raw extractions saved to: {extraction_checkpoint_path}\")\n",
    "print(f\"   File size: {extraction_checkpoint_path.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Extraction Quality Review\n",
    "\n",
    "Inspect extracted text for quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "extraction_stats = []\n",
    "\n",
    "for extraction in raw_extractions:\n",
    "    if extraction['status'] == 'success':\n",
    "        extraction_stats.append({\n",
    "            \"Doc ID\": extraction['doc_id'],\n",
    "            \"Doc Name\": extraction['doc_name'],\n",
    "            \"Pages\": extraction['page_count'],\n",
    "            \"Characters\": f\"{extraction['total_char_count']:,}\",\n",
    "            \"Avg Chars/Page\": f\"{extraction['total_char_count'] // extraction['page_count']:,}\",\n",
    "            \"Status\": \"‚úì Success\"\n",
    "        })\n",
    "    else:\n",
    "        extraction_stats.append({\n",
    "            \"Doc ID\": extraction['doc_id'],\n",
    "            \"Doc Name\": extraction['doc_name'],\n",
    "            \"Pages\": \"N/A\",\n",
    "            \"Characters\": \"N/A\",\n",
    "            \"Avg Chars/Page\": \"N/A\",\n",
    "            \"Status\": \"‚úó Failed\"\n",
    "        })\n",
    "\n",
    "stats_df = pd.DataFrame(extraction_stats)\n",
    "print(\"\\nüìä Extraction Statistics:\\n\")\n",
    "display(stats_df)\n",
    "\n",
    "# Overall stats\n",
    "successful = [e for e in raw_extractions if e['status'] == 'success']\n",
    "if successful:\n",
    "    total_pages = sum(e['page_count'] for e in successful)\n",
    "    total_chars = sum(e['total_char_count'] for e in successful)\n",
    "    print(f\"\\nüìà Overall:\")\n",
    "    print(f\"   Total pages extracted: {total_pages}\")\n",
    "    print(f\"   Total characters: {total_chars:,}\")\n",
    "    print(f\"   Average document size: {total_chars // len(successful):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample text from each document\n",
    "print(\"\\nüìù Sample Text from Each Document:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for extraction in raw_extractions:\n",
    "    if extraction['status'] == 'success':\n",
    "        print(f\"\\n{extraction['doc_id']} - {extraction['doc_name']}\")\n",
    "        print(\"-\" * 80)\n",
    "        # Show first 500 characters\n",
    "        sample_text = extraction['full_text'][:500]\n",
    "        print(sample_text)\n",
    "        if len(extraction['full_text']) > 500:\n",
    "            print(\"\\n[... truncated ...]\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality checks\n",
    "print(\"\\nüîç Quality Checks:\\n\")\n",
    "\n",
    "quality_issues = []\n",
    "\n",
    "for extraction in raw_extractions:\n",
    "    if extraction['status'] == 'success':\n",
    "        text = extraction['full_text']\n",
    "        doc_id = extraction['doc_id']\n",
    "        \n",
    "        # Check for potential issues\n",
    "        issues = []\n",
    "        \n",
    "        # Very short extraction\n",
    "        if len(text) < 100:\n",
    "            issues.append(\"Very short text (< 100 chars)\")\n",
    "        \n",
    "        # Check for excessive special characters (possible OCR issues)\n",
    "        special_char_ratio = sum(1 for c in text if not c.isalnum() and not c.isspace()) / len(text)\n",
    "        if special_char_ratio > 0.3:\n",
    "            issues.append(f\"High special char ratio ({special_char_ratio:.1%})\")\n",
    "        \n",
    "        # Check for repeated characters (OCR artifact)\n",
    "        if '.....' in text or '-----' in text:\n",
    "            issues.append(\"Repeated characters detected\")\n",
    "        \n",
    "        if issues:\n",
    "            quality_issues.append({\n",
    "                \"Doc ID\": doc_id,\n",
    "                \"Issues\": \"; \".join(issues)\n",
    "            })\n",
    "\n",
    "if quality_issues:\n",
    "    quality_df = pd.DataFrame(quality_issues)\n",
    "    print(\"‚ö†Ô∏è Potential Quality Issues Detected:\\n\")\n",
    "    display(quality_df)\n",
    "else:\n",
    "    print(\"‚úì No obvious quality issues detected\")\n",
    "    print(\"   Note: Manual review of sample text above is still recommended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Text Chunking\n",
    "\n",
    "Apply LangChain RecursiveCharacterTextSplitter to create chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk all successfully extracted documents\n",
    "print(\"\\nüîÑ Starting chunking process...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "for extraction in raw_extractions:\n",
    "    if extraction['status'] == 'success':\n",
    "        print(f\"\\nChunking: {extraction['doc_id']} - {extraction['doc_name']}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        chunks = processor.chunk_document(extraction)\n",
    "        all_chunks.extend(chunks)\n",
    "        \n",
    "        print(f\"  ‚úì Created {len(chunks)} chunks\")\n",
    "        if chunks:\n",
    "            avg_chunk_size = sum(c['char_count'] for c in chunks) / len(chunks)\n",
    "            print(f\"  ‚úì Average chunk size: {avg_chunk_size:.0f} characters\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\n‚úì Chunking complete: {len(all_chunks)} total chunks created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Chunk Analysis\n",
    "\n",
    "Analyze chunk distribution and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk distribution by document\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if all_chunks:\n",
    "    # Group chunks by document\n",
    "    chunks_by_doc = {}\n",
    "    for chunk in all_chunks:\n",
    "        doc_id = chunk['doc_id']\n",
    "        if doc_id not in chunks_by_doc:\n",
    "            chunks_by_doc[doc_id] = []\n",
    "        chunks_by_doc[doc_id].append(chunk)\n",
    "    \n",
    "    # Create distribution table\n",
    "    distribution_data = []\n",
    "    for doc_id, chunks in chunks_by_doc.items():\n",
    "        chunk_sizes = [c['char_count'] for c in chunks]\n",
    "        distribution_data.append({\n",
    "            \"Doc ID\": doc_id,\n",
    "            \"Chunk Count\": len(chunks),\n",
    "            \"Avg Size\": f\"{sum(chunk_sizes) / len(chunks):.0f}\",\n",
    "            \"Min Size\": min(chunk_sizes),\n",
    "            \"Max Size\": max(chunk_sizes),\n",
    "            \"Total Chars\": f\"{sum(chunk_sizes):,}\"\n",
    "        })\n",
    "    \n",
    "    dist_df = pd.DataFrame(distribution_data)\n",
    "    print(\"\\nüìä Chunk Distribution by Document:\\n\")\n",
    "    display(dist_df)\n",
    "    \n",
    "    # Visualize chunk size distribution\n",
    "    all_chunk_sizes = [c['char_count'] for c in all_chunks]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(all_chunk_sizes, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.axvline(ProcessingConfig.CHUNK_SIZE, color='red', linestyle='--', label=f'Target: {ProcessingConfig.CHUNK_SIZE}')\n",
    "    plt.xlabel('Chunk Size (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Chunk Size Distribution')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    doc_ids = [d[\"Doc ID\"] for d in distribution_data]\n",
    "    chunk_counts = [d[\"Chunk Count\"] for d in distribution_data]\n",
    "    plt.bar(doc_ids, chunk_counts, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Document ID')\n",
    "    plt.ylabel('Number of Chunks')\n",
    "    plt.title('Chunks per Document')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(checkpoint_dir / 'chunk_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìà Visualization saved to: {checkpoint_dir / 'chunk_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example chunks\n",
    "print(\"\\nüìù Example Chunks:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show first chunk from each document\n",
    "if all_chunks:\n",
    "    docs_shown = set()\n",
    "    for chunk in all_chunks:\n",
    "        if chunk['doc_id'] not in docs_shown:\n",
    "            docs_shown.add(chunk['doc_id'])\n",
    "            print(f\"\\n{chunk['chunk_id']}\")\n",
    "            print(f\"Doc: {chunk['doc_name']}, Pages: {chunk['page_numbers']}, Size: {chunk['char_count']} chars\")\n",
    "            print(\"-\" * 80)\n",
    "            # Show first 300 characters\n",
    "            print(chunk['text'][:300])\n",
    "            if len(chunk['text']) > 300:\n",
    "                print(\"\\n[... truncated ...]\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk boundary analysis\n",
    "print(\"\\nüîç Chunk Boundary Analysis:\\n\")\n",
    "\n",
    "if all_chunks:\n",
    "    # Check for very small chunks (might indicate issues)\n",
    "    small_chunks = [c for c in all_chunks if c['char_count'] < ProcessingConfig.CHUNK_SIZE * 0.3]\n",
    "    \n",
    "    # Check for chunks at max size (might be cut mid-sentence)\n",
    "    large_chunks = [c for c in all_chunks if c['char_count'] >= ProcessingConfig.CHUNK_SIZE * 0.95]\n",
    "    \n",
    "    print(f\"Small chunks (< 30% of target): {len(small_chunks)} / {len(all_chunks)} ({len(small_chunks)/len(all_chunks)*100:.1f}%)\")\n",
    "    print(f\"Large chunks (>= 95% of target): {len(large_chunks)} / {len(all_chunks)} ({len(large_chunks)/len(all_chunks)*100:.1f}%)\")\n",
    "    \n",
    "    # Check overlap effectiveness\n",
    "    print(f\"\\nOverlap check: Comparing consecutive chunks...\")\n",
    "    overlap_examples = 0\n",
    "    for i in range(min(3, len(all_chunks) - 1)):\n",
    "        chunk1 = all_chunks[i]\n",
    "        chunk2 = all_chunks[i + 1]\n",
    "        if chunk1['doc_id'] == chunk2['doc_id']:  # Same document\n",
    "            # Check if there's any overlap\n",
    "            end_of_first = chunk1['text'][-100:]  # Last 100 chars of first chunk\n",
    "            start_of_second = chunk2['text'][:100]  # First 100 chars of second chunk\n",
    "            \n",
    "            # Simple overlap detection\n",
    "            overlap_found = any(end_of_first[i:i+20] in start_of_second for i in range(len(end_of_first)-20))\n",
    "            \n",
    "            if overlap_found:\n",
    "                overlap_examples += 1\n",
    "    \n",
    "    print(f\"  Overlap detected in {overlap_examples}/3 sample consecutive chunk pairs\")\n",
    "    print(f\"  (Expected behavior with overlap={ProcessingConfig.CHUNK_OVERLAP})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Final Output Preparation\n",
    "\n",
    "Validate and save processed chunks for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema validation\n",
    "print(\"\\n‚úÖ Schema Validation:\\n\")\n",
    "\n",
    "required_fields = ['chunk_id', 'doc_id', 'doc_name', 'text', 'chunk_index', 'char_count', 'page_numbers']\n",
    "\n",
    "validation_passed = True\n",
    "for chunk in all_chunks[:5]:  # Check first 5 chunks\n",
    "    missing_fields = [field for field in required_fields if field not in chunk]\n",
    "    if missing_fields:\n",
    "        print(f\"‚úó Chunk {chunk.get('chunk_id', 'unknown')} missing fields: {missing_fields}\")\n",
    "        validation_passed = False\n",
    "\n",
    "if validation_passed:\n",
    "    print(\"‚úì All chunks have required fields\")\n",
    "    print(f\"‚úì Schema: {', '.join(required_fields)}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Warning: Some chunks have missing fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final output format\n",
    "# Add any additional metadata or formatting needed for OpenSearch\n",
    "\n",
    "final_chunks = []\n",
    "for chunk in all_chunks:\n",
    "    # Create a clean version for indexing\n",
    "    final_chunk = {\n",
    "        'id': chunk['chunk_id'],  # Use as document ID in OpenSearch\n",
    "        'text': chunk['text'],\n",
    "        'metadata': {\n",
    "            'doc_id': chunk['doc_id'],\n",
    "            'doc_name': chunk['doc_name'],\n",
    "            'chunk_index': chunk['chunk_index'],\n",
    "            'char_count': chunk['char_count'],\n",
    "            'page_numbers': chunk['page_numbers'],\n",
    "            'processing_timestamp': chunk['processing_timestamp']\n",
    "        }\n",
    "    }\n",
    "    final_chunks.append(final_chunk)\n",
    "\n",
    "print(f\"‚úì Prepared {len(final_chunks)} chunks for indexing\")\n",
    "print(f\"\\nSample output format:\")\n",
    "print(json.dumps(final_chunks[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed chunks\n",
    "chunks_output_path = checkpoint_dir / ProcessingConfig.PROCESSED_CHUNKS_FILE\n",
    "\n",
    "with open(chunks_output_path, 'w') as f:\n",
    "    json.dump(final_chunks, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Processed chunks saved to: {chunks_output_path}\")\n",
    "print(f\"   File size: {chunks_output_path.stat().st_size / 1024:.2f} KB\")\n",
    "print(f\"   Total chunks: {len(final_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã PROCESSING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "successful_extractions = [e for e in raw_extractions if e['status'] == 'success']\n",
    "\n",
    "print(f\"\\nüìÑ Document Processing:\")\n",
    "print(f\"   Total documents: {len(pdf_sources)}\")\n",
    "print(f\"   Successfully extracted: {len(successful_extractions)}\")\n",
    "print(f\"   Failed extractions: {len(pdf_sources) - len(successful_extractions)}\")\n",
    "\n",
    "if successful_extractions:\n",
    "    total_pages = sum(e['page_count'] for e in successful_extractions)\n",
    "    total_chars = sum(e['total_char_count'] for e in successful_extractions)\n",
    "    print(f\"   Total pages: {total_pages}\")\n",
    "    print(f\"   Total characters: {total_chars:,}\")\n",
    "\n",
    "print(f\"\\nüì¶ Chunk Generation:\")\n",
    "print(f\"   Total chunks created: {len(all_chunks)}\")\n",
    "if all_chunks:\n",
    "    avg_chunk_size = sum(c['char_count'] for c in all_chunks) / len(all_chunks)\n",
    "    print(f\"   Average chunk size: {avg_chunk_size:.0f} characters\")\n",
    "    print(f\"   Target chunk size: {ProcessingConfig.CHUNK_SIZE} characters\")\n",
    "    print(f\"   Chunk overlap: {ProcessingConfig.CHUNK_OVERLAP} characters\")\n",
    "\n",
    "print(f\"\\nüíæ Outputs:\")\n",
    "print(f\"   Config: {checkpoint_dir / 'config.json'}\")\n",
    "print(f\"   Raw extractions: {checkpoint_dir / ProcessingConfig.RAW_EXTRACTION_FILE}\")\n",
    "print(f\"   Processed chunks: {checkpoint_dir / ProcessingConfig.PROCESSED_CHUNKS_FILE}\")\n",
    "print(f\"   Visualization: {checkpoint_dir / 'chunk_distribution.png'}\")\n",
    "\n",
    "print(f\"\\n‚è≠Ô∏è  Next Steps:\")\n",
    "print(f\"   1. Review the sample chunks above for quality\")\n",
    "print(f\"   2. If satisfied, proceed to: 02_indexing.ipynb\")\n",
    "print(f\"   3. If adjustments needed, modify config and re-run from Section 7\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì Notebook execution complete\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notes & Observations\n",
    "\n",
    "**Document Quality:**\n",
    "- [Add observations about extraction quality]\n",
    "- [Note any problematic documents]\n",
    "\n",
    "**Chunking Strategy:**\n",
    "- [Document why you chose these chunk parameters]\n",
    "- [Note any adjustments made during experimentation]\n",
    "\n",
    "**Issues Encountered:**\n",
    "- [List any errors or unexpected behavior]\n",
    "\n",
    "**Next Experiments:**\n",
    "- [Ideas for different chunk sizes]\n",
    "- [Alternative splitting strategies]\n",
    "- [Metadata enhancements]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
